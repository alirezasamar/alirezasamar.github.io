[{"content":"Gradient Descent Algorithm in Machine Learning Gradient descent is an optimization algorithm used in machine learning to minimize a loss function, which quantifies the difference between predicted and actual outcomes. It iteratively adjusts model parameters to minimize the error between predictions and actual data. In this post, we\u0026rsquo;ll provide a simple, technical explanation of gradient descent, and discuss how it works in a machine learning context.\nOverview Consider a loss function, L(w), which measures the error between the predicted and actual outcomes. The goal is to find the optimal values of the weights w that minimize the loss function. Gradient descent achieves this by iteratively adjusting the weights in the direction of the steepest decrease in the loss function, which is given by the negative of the gradient.\nThe gradient of the loss function, denoted by ∇L(w), is a vector of partial derivatives with respect to each weight. The gradient points in the direction of the steepest increase in the function, so we update the weights by subtracting the gradient times a learning rate α. The learning rate determines the step size of the update and is a hyperparameter that needs to be chosen carefully.\nThe update rule for gradient descent is:\n$$ (w_{\\text{new}} = w_{\\text{old}} - \\alpha \\nabla L(w_{\\text{old}})) $$\nThis process is repeated until convergence, which can be determined by a variety of stopping criteria, such as reaching a maximum number of iterations, or when the change in the loss function between iterations falls below a predefined threshold.\nTypes of Gradient Descent There are three main types of gradient descent, which differ in how they calculate the gradient:\nBatch Gradient Descent: Computes the gradient using the entire dataset. This can be computationally expensive for large datasets, but provides an accurate estimate of the gradient.\n$$ (\\nabla L(w) = \\frac{1}{N} \\sum_{i=1}^N [\\nabla l_i(w)]) $$\nStochastic Gradient Descent (SGD): Computes the gradient using a single randomly selected data point per iteration. This makes it computationally efficient, but introduces noise in the gradient estimation.\n$$ (\\nabla L(w) = \\nabla l_i(w) \\quad \\text{for a randomly chosen } i) $$\nMini-batch Gradient Descent: Computes the gradient using a small subset (mini-batch) of the dataset. This balances the computational efficiency of SGD with the accuracy of batch gradient descent.\n$$ (\\nabla L(w) = \\frac{1}{|B|} \\sum_{i \\in B} [\\nabla l_i(w)]) $$\nExample: Linear Regression Let\u0026rsquo;s consider a simple example of linear regression, where we want to fit a line to a set of data points. The linear model is given by:\n$$ (y_{\\text{pred}} = w_0 + w_1 \\cdot x) $$\nWe use the mean squared error (MSE) as our loss function:\n$$ (L(w) = \\frac{1}{N} \\sum_{i=1}^N [(y_i - y_{\\text{pred}_i})^2]) $$\nTo perform gradient descent, we need to compute the gradient of the MSE with respect to the weights w_0 and w_1:\n$$ (\\nabla L(w) = \\left[ \\frac{\\partial L}{\\partial w_0}, \\frac{\\partial L}{\\partial w_1} \\right]) $$\nAfter calculating the partial derivatives, we get:\n$$ (\\frac{\\partial L}{\\partial w_0} = \\frac{-2}{N} \\sum_{i=1}^N [(y_i - y_{\\text{pred}_i})]) $$\n$$ (\\frac{\\partial L}{\\partial w_1} = \\frac{-2}{N} \\sum_{i=1}^N [(y_i - y_{\\text{pred}_i}) \\cdot x_i]) $$\nNow, we can apply the gradient descent update rule for each weight:\n$$ (w_{0_{\\text{new}}} = w_{0_{\\text{old}}} - \\alpha \\frac{\\partial L}{\\partial w_0}) $$\n$$ (w_{1_{\\text{new}}} = w_{1_{\\text{old}}} - \\alpha \\frac{\\partial L}{\\partial w_1}) $$\nWe repeat these updates until convergence or until we reach the maximum number of iterations.\nPseudocode Here\u0026rsquo;s a simple pseudocode for implementing gradient descent on a linear regression problem:\n1. Initialize the weights w_0 and w_1 with random values 2. Set the learning rate α and the maximum number of iterations 3. Repeat until convergence or maximum iterations: a. Calculate the gradient of the loss function with respect to the weights: ∂L/∂w_0 and ∂L/∂w_1 b. Update the weights using the gradient descent rule: w_0_new = w_0_old - α * ∂L/∂w_0 w_1_new = w_1_old - α * ∂L/∂w_1 c. Check convergence criteria Summary Gradient descent is a powerful optimization algorithm used in machine learning to minimize loss functions. It works by iteratively updating model parameters in the direction of the steepest decrease in the loss function. There are three main types of gradient descent: batch, stochastic, and mini-batch. In this post, we discussed the algorithm in the context of linear regression, but gradient descent can be applied to a wide variety of machine learning problems.\n","permalink":"https://alirezasamar.com/posts/gradient-descent-simplified/","summary":"Gradient Descent Algorithm in Machine Learning Gradient descent is an optimization algorithm used in machine learning to minimize a loss function, which quantifies the difference between predicted and actual outcomes. It iteratively adjusts model parameters to minimize the error between predictions and actual data. In this post, we\u0026rsquo;ll provide a simple, technical explanation of gradient descent, and discuss how it works in a machine learning context.\nOverview Consider a loss function, L(w), which measures the error between the predicted and actual outcomes.","title":"Gradient Descent Simplified"},{"content":"The world of machine learning has seen rapid advancements in recent years, leading to the widespread adoption of ML models across various industries. One key development in this domain is distributed federated learning, a decentralized approach that facilitates the efficient training and application of ML models at scale. In this blog, we will discuss the importance of distributed federated learning for businesses and its potential to revolutionize the way industries leverage ML models.\nIllustration by Shubham Dhage\nWhat is Distributed Federated Learning? Distributed federated learning (DFL) is a technique that enables ML models to be trained on data distributed across multiple devices, while maintaining data privacy and security. Instead of centralizing the data, DFL allows the model to learn from the distributed data sources, making updates at the edge devices and subsequently aggregating them in a central server. This approach provides several benefits, including reduced communication costs, enhanced privacy, and increased model performance.\nWhy Distributed Federated Learning Matters for Businesses 1. Data Privacy and Security With the growing concern around data privacy and the stringent regulations that govern it, businesses must ensure the protection of their sensitive data. DFL helps address this challenge by keeping the data on the edge devices and training the ML models locally, reducing the need for data to be transmitted to a central location. This method not only minimizes the risk of data breaches but also complies with data protection regulations.\n2. Scalability and Resource Efficiency As businesses continue to grow, the volume and complexity of their data increase. DFL allows companies to efficiently scale their ML models by distributing the training process across multiple devices, reducing the burden on a central server. This not only saves computational resources but also accelerates the training process, enabling organizations to handle large-scale ML projects effectively.\n3. Collaborative Learning and Enhanced Model Performance Distributed federated learning enables businesses to leverage the collective intelligence of multiple data sources. By training the models on diverse datasets, DFL results in more accurate and robust models that generalize better to unseen data. This collaborative approach allows industries to benefit from the combined knowledge of different organizations, leading to improved overall model performance.\n4. Reduced Latency and Improved User Experience In a DFL setup, since the ML models are trained and updated at the edge devices, the inference latency is significantly reduced. This ensures a seamless user experience and enables real-time decision-making capabilities. Moreover, with the continuous improvement of models at the device level, the overall system performance is optimized, making it an ideal solution for industries that demand real-time analytics and insights.\nDistributed Federated Learning in Action Many industries are already reaping the benefits of DFL. For instance, in the healthcare sector, DFL is being used to develop ML models that can predict and diagnose diseases without compromising patient privacy. In the automotive industry, companies are leveraging DFL to create smart, connected vehicles that can communicate with each other and share knowledge, improving safety and efficiency.\nSummary Distributed federated learning is revolutionizing the way businesses adopt and implement ML models at scale. By offering enhanced data privacy, resource efficiency, and collaborative learning, DFL is becoming the go-to solution for businesses that seek to harness the power of machine learning in a secure and scalable manner. As industries continue to embrace this decentralized approach, it\u0026rsquo;s only a matter of time before DFL becomes an integral part of the modern business landscape.\n","permalink":"https://alirezasamar.com/posts/embracing-distributed-federated-learning-for-scalable-business-solutions/","summary":"The world of machine learning has seen rapid advancements in recent years, leading to the widespread adoption of ML models across various industries. One key development in this domain is distributed federated learning, a decentralized approach that facilitates the efficient training and application of ML models at scale. In this blog, we will discuss the importance of distributed federated learning for businesses and its potential to revolutionize the way industries leverage ML models.","title":"Embracing Distributed Federated Learning for Scalable Business Solutions"},{"content":"Fine-tuning a pre-trained model for an image classification task on a domain-specific problem can significantly reduce the time and computational resources required for training a deep neural network from scratch. In this tutorial, I will walk through the steps of fine-tuning a pre-trained ResNet-18 model on a custom dataset using PyTorch.\nStep 1: Load the pre-trained model let\u0026rsquo;s start by loading the pre-trained ResNet-18 model from the PyTorch model zoo. The PyTorch model zoo provides a collection of pre-trained models that can be used for various computer vision tasks.\nimport torch import torchvision.models as models # Load the pre-trained ResNet-18 model model = models.resnet18(pretrained=True) Step 2: Freeze the pre-trained layers Next, freeze the pre-trained layers of the ResNet-18 model so that we can only train the last few layers for our specific task. This is done to prevent the weights of the pre-trained layers from being updated during training.\n# Freeze all the pre-trained layers for param in model.parameters(): param.requires_grad = False Step 3: Modify the last layer The last layer of the pre-trained ResNet-18 model is a fully connected layer that outputs a 1000-dimensional vector. Since the main idea here is to use this model for a domain-specific image classification task, we need to modify the last layer to output the number of classes in our dataset.\n# Modify the last layer of the model num_classes = 10 # replace with the number of classes in your dataset model.fc = torch.nn.Linear(model.fc.in_features, num_classes) Step 4: Load the custom dataset Next, load the custom dataset that you want to use for training the fine-tuned model. In this example, I will assume that the dataset is organized in the following directory structure:\ncustom_dataset/ ├── train/ │ ├── class1/ │ ├── class2/ │ ├── ... ├── val/ │ ├── class1/ │ ├── class2/ │ ├── ... I will use the PyTorch ImageFolder dataset class to load the dataset.\nfrom torchvision.datasets import ImageFolder from torchvision.transforms import transforms # Define the transformations to apply to the images transform = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) # Load the train and validation datasets train_dataset = ImageFolder(\u0026#39;custom_dataset/train\u0026#39;, transform=transform) val_dataset = ImageFolder(\u0026#39;custom_dataset/val\u0026#39;, transform=transform) Step 5: Define the loss function and optimizer I will use the cross-entropy loss function and the stochastic gradient descent (SGD) optimizer for training the fine-tuned model.\n# Define the loss function and optimizer criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9) Step 6: Train the model Finally, train the fine-tuned model on the custom dataset using PyTorch\u0026rsquo;s DataLoader and TrainLoader utilities.\nfrom torch.utils.data import DataLoader # Create data loaders for the train and validation datasets train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) Next, define a function to train the fine-tuned model for a specified number of epochs.\ndef train(model, train_loader, val_loader, criterion, optimizer, num_epochs): # Train the model for the specified number of epochs for epoch in range(num_epochs): # Set the model to train mode model.train() # Initialize the running loss and accuracy running_loss = 0.0 running_corrects = 0 # Iterate over the batches of the train loader for inputs, labels in train_loader: # Move the inputs and labels to the device inputs = inputs.to(device) labels = labels.to(device) # Zero the optimizer gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # Backward pass and optimizer step loss.backward() optimizer.step() # Update the running loss and accuracy running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) # Calculate the train loss and accuracy train_loss = running_loss / len(train_dataset) train_acc = running_corrects.double() / len(train_dataset) # Set the model to evaluation mode model.eval() # Initialize the running loss and accuracy running_loss = 0.0 running_corrects = 0 # Iterate over the batches of the validation loader with torch.no_grad(): for inputs, labels in val_loader: # Move the inputs and labels to the device inputs = inputs.to(device) labels = labels.to(device) # Forward pass outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # Update the running loss and accuracy running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) # Calculate the validation loss and accuracy val_loss = running_loss / len(val_dataset) val_acc = running_corrects.double() / len(val_dataset) # Print the epoch results print(\u0026#39;Epoch [{}/{}], train loss: {:.4f}, train acc: {:.4f}, val loss: {:.4f}, val acc: {:.4f}\u0026#39; .format(epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc)) Step 7: Fine-tune the model on the custom dataset Let\u0026rsquo;s now fine-tune the pre-trained ResNet-18 model on the custom dataset by training the last layer for a few epochs and then unfreezing all the layers and training the entire network for a few more epochs.\n# Set the device device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) model.to(device) # Fine-tune the last layer for a few epochs optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9) train(model, train_loader, val_loader, criterion, optimizer, num_epochs=5) # Unfreeze all the layers and fine-tune the entire network for a few more epochs for param in model.parameters(): param.requires_grad = True optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10) Summary In this tutorial, I described how to fine-tune a pre-trained ResNet-18 model for a domain-specific image classification task using PyTorch. I loaded the pre-trained model, froze the pre-trained layers, modified the last layer, loaded the custom dataset, defined the loss function and optimizer, and trained the fine-tuned model for a specified number of epochs. I also fine-tuned the last layer for a few epochs and then unfroze all the layers and fine-tuned the entire network for a few more epochs.\nFine-tuning pre-trained models is a powerful technique that can significantly reduce the time and computational resources required for training deep neural networks for specific tasks. It is also a common approach in production environments where the datasets are usually small and the training time is limited.\nThe code snippets provided in this tutorial can serve as a starting point for fine-tuning pre-trained models on domain-specific image classification tasks. However, the approach may need to be adapted to different datasets and tasks, and hyperparameters may need to be tuned for optimal performance.\n","permalink":"https://alirezasamar.com/posts/fine-tuning-pre-trained-resnet-18-model-image-classification-pytorch/","summary":"Fine-tuning a pre-trained model for an image classification task on a domain-specific problem can significantly reduce the time and computational resources required for training a deep neural network from scratch. In this tutorial, I will walk through the steps of fine-tuning a pre-trained ResNet-18 model on a custom dataset using PyTorch.\nStep 1: Load the pre-trained model let\u0026rsquo;s start by loading the pre-trained ResNet-18 model from the PyTorch model zoo. The PyTorch model zoo provides a collection of pre-trained models that can be used for various computer vision tasks.","title":"Fine-Tuning a Pre-Trained ResNet-18 Model for Image Classification with PyTorch"},{"content":" Photo by Nicolas Hoizey\nModern software engineering is all about shipping fast and often\nIt was about a month ago that I was introduced to a book called Accelerate and later discovered another study Accelerate State of DevOps Report.\nIt is all natural to agile teams today but what all these studies demonstrate is that engineering teams that deliver more often are more likely to meet or exceed their organizational performance goals.\nI remember when I was in a demo meeting with a large organization and when they discovered we deployed sometimes over 5 times in a single day, their first impression was “okay, your software is not stable!” this claim was totally against our uptime rate, 99.99% on all servers!\nShipping fast and often has numerous advantages, it is a simple logic actually, frequent releases mean smaller batches being deployed therefore less risk of failure or downtime. In the event of failure, fast releases mean fast recovery from failure.\nIn your team in order to have high performers, they need to be able to quickly get feedback on their work quickly. This is where code review comes in.\nThe benefits of code review are many. First and foremost, it helps catch mistakes. Having someone else look at the code can help you find errors that may have been missed at that pace. It can also help find cleaner and optimal ways to write code and make code more readable.\nCode review also helps to build a shared understanding of the codebase to be able to recover from failure events. By having everyone on the team look at the code, they will all be familiar with it and will be able to make changes to it more easily.\nFinally, code review helps to build a culture of collaboration and feedback in your team.\nIn short, it took a long time for software engineering teams to understand traditional approaches such as the Waterfall model might not work best in software development as it is beneficial for additional engineering. Simple because building pace doesn’t have a correlation with the size of a potential failure, hence allowing you to recover faster!\nThis is a very controversial topic in software engineering and engineering management, what is your experience?\n","permalink":"https://alirezasamar.com/posts/continuous-delivery-champions/","summary":"Photo by Nicolas Hoizey\nModern software engineering is all about shipping fast and often\nIt was about a month ago that I was introduced to a book called Accelerate and later discovered another study Accelerate State of DevOps Report.\nIt is all natural to agile teams today but what all these studies demonstrate is that engineering teams that deliver more often are more likely to meet or exceed their organizational performance goals.","title":"Continuous Delivery Champions"},{"content":" Photo by Matteo Vistocco\nThe role of the data science manager is to provide group-based leadership with a data science focus. The data science manager must have a deep understanding of the data science process and the ability to lead and motivate data science teams. The data science manager must also work with business stakeholders to understand their needs and translate them into data science solutions.\nThe data science manager is responsible for the following:\nLeading and motivating data science teams Ensuring that data science solutions are aligned with business needs Coordinating data science work with other parts of the organization The data science manager must have the following skills:\nLeadership and management skills Technical skills in data science Business skills to understand and communicate with business stakeholders. The data science manager role is a new and growing role in the data science field. As data science becomes more important in business, the data science manager role will become more critical. But what are the components that make a data scientist manager? In this post, I am trying to discuss a few that I found the most relevant based on my experience in the past few years.\nBe honest with people and willing to get hands dirty This approach also promotes open communication between all parties. We’re more likely to uncover practical answers when the team feels comfortable introducing new ideas and discussing difficulties openly. No one has a monopoly on good ideas, and the idea hierarchy should be flat.\nOpenly disclosing company KPIs and performance data is also critical. People’s perspectives shift when they see their direct impact on the firm’s bottom line. They become more inventive, interested, and eager to try new things.\nSet parameters Being a data science leader necessitates regular team meetings, idea-generating sessions, and strategic project management. While data science directors may not be as hands-on as their team members, they cannot afford to ignore the day-to-day tasks. The position requires the leader to take on a new function, which includes gathering requirements, establishing general parameters, and providing the appropriate amount of feedback.\nGive feedback often and help your team understand where they are and how they can grow, but don’t forget to ask for feedback. Yes! You need feedback to align yourself with the team and adjust your path.\nMutual trust To ensure that continuous experimentation is successful, we must have faith that the team is working diligently every day to tackle the problems that have been entrusted to them. We must also have confidence that we have assembled the most incredible team possible. On the other hand, the team should realize that we are working as hard for them as they are for us. The group becomes motivated to explore and succeed due to developing mutual trust.\nA big step toward trust is the feeling of being involved in decision-making. It is a normal human desire to be heard, so before making any decision, ask each of your team members their opinions and give them a chance to speak out regardless of their level on the ladder. It is usually impossible to oblige all the voices, but you can contextualize decisions, and your team will know that their opinion will be considered.\nHowever, trust should not be blind. We can see the code while people are developing using software systems. At Companio, my team manages production-level techniques such as intelligent lead routing and lead bidding algorithms. I can see if my team is innovating and developing new approaches directly.\nStay curious A good leader should look for new methods and think about how they may be used in the team’s work.\nGrowth will be limited if there is no curiosity. We are essentially establishing an atmosphere favouring high-impact experiments when we foster a culture of creativity in our data science departments. Many data scientists are naturally curious, so giving them the freedom to explore the depths of their thoughts is critical. Our curiosity as leaders influences how the team approaches daily difficulties and fuels their collective creative energy in constructing forward-thinking solutions.\nKeep yourself informed about the higher business settings In many companies, a data science team’s work has a ripple effect throughout the organization; our work interacts with other company-wide systems in potentially complex ways. We might improve a system that we think is isolated without knowing about systems integration. Still, such actions could have a cascading effect on other systems, processes, and people.\nIt’s critical to connect the overall impact and context to the bigger picture. So make sure every team member knows how their work impacts and helps others and the company as this is the best motivator.\nSummary Up to this point of this article, you have noticed the golden key is communication, or as I’d like to call it sometimes, “over-communication”.\nManagers should be the most prominent advocate for the team members to thrive and grow, and transparent communication is the route to this goal.\nA simple role in mind is that if you grew your team member to the point that they became a clear replacement for your position, you did your job very well as a forward-thinking leader. It might sound rough at first but try to sleep on it a bit, and you will see a true leader definition appears.\n","permalink":"https://alirezasamar.com/posts/key-components-of-an-effective-data-science-manager/","summary":"Photo by Matteo Vistocco\nThe role of the data science manager is to provide group-based leadership with a data science focus. The data science manager must have a deep understanding of the data science process and the ability to lead and motivate data science teams. The data science manager must also work with business stakeholders to understand their needs and translate them into data science solutions.\nThe data science manager is responsible for the following:","title":"Key Components of An Effective Data Science Manager"},{"content":" Recommender systems are everywhere. They are helpful but sometimes biased. But building a recommender system which is for sure an application of machine learning in production requires some special treatments and aspects to cover in comparison to other machine learning application like let\u0026rsquo;s say classification.\nIn this article I will go through three of these aspects:\nPhoto by Javier Allegue Barros\n1. Model Selection How do you choose the model? What constitutes a good model? Let\u0026rsquo;s assume you need a classification model. Pretty easy to find a baseline, right? Any classification problem statement that you might have, the random forest is probably going to work reasonably well. Unless you have images, in which case you might go with a convolutional neural network, and wallah there you have it. But things are different in recommender systems! If you do a survey, you will find several standards approaches, such as the class of models that won the Netflix prize called collaborative filtering models, in which it uses information about how people buy or use your items to recommend other items. But these could be useless for some applications, and the fact that Amazon and Netflix are using these sort of models doesn\u0026rsquo;t mean they work perfectly well for you as well. This requires a careful and detailed look at your problem statement. If novelty is an important component of your problem like if you need to recommend news articles, a collaborative filtering approach might be completely useless to you. In collaborative filtering approach when you want to recommend a news item based on another news item, you look what sort of users looks at these two news items. If a given set of users looked at item A and the same set of users looks at item B that means these two items are similar and if a user looks at item A you should probably recommend them item B. The problem here is that news is an area where novelty is important, and if you use collaborative filtering and the article is new, nobody looked at it, so no data to rate the recommendation and you end up not having any good result at the end. Another areas to be considered is the popularity and continuation. For instance, if you watched a TV series and you come back to the site again, the recommendation should take into account that you might continue watching the series, or if you went to an e-commerce site and viewed and bookmarked few items, chances are that you come back to purchase them are high. Another factor could be metadata. Let\u0026rsquo;s recall that news items recommendation problem; You know a little bit about the user, like what sort of interest they have. So if somebody is interested in sports, and you know the given article is about the sport with its metadata, even though it\u0026rsquo;s new you have now something to go on. If you don\u0026rsquo;t know what is the right model or indeed if there is a right one, ensemble! Mix different models to cater to different user interactions. But with all the above, how do you know a given model is better than another? This question takes us to the second main difference between RS and other ML applications.\n2. Ensemble, ensemble, ensemble How do you know one model is better than another? In recommendation systems, there is a technique called ranking metrics which defines the goal of your system is to compute a ranked list of items to recommend to a given user and a good model is the one that put users interest at the top of this list. The usual way is offline testing is to take the interaction data, split it to a training and testing set, feed the training to model and if the result shows users interests at top of the list in comparison to test set, then you are all good. Even though you have all these metrics and you have to use them, but the evaluation is still difficult. What do you even measure? Browse? Click? Purchase? What is the expression of interest? Like if you are a fashion eCommerce, people might save a lot of items (worth of thousands of dollars) but this is more like an aspiration to them than the expression of interest to purchase. Even if you take purchases data, that\u0026rsquo;s also sparse, since people didn\u0026rsquo;t buy a lot of items and that\u0026rsquo;s not enough data to train a model. So, any of these on their own aren\u0026rsquo;t good evaluation metrics, and you need to think about combining those factors that matter to you. Is the future like the past? It\u0026rsquo;s a simply difficult challenge for recommender systems. You get some data for training, you train your model and from tomorrow onwards you will have totally different data to feed to your model! Recalling that fashion eCommerce, imagine you have a collection of dresses and they are going to be immensely popular soon after they\u0026rsquo;re released. RS starts to recommend those items but this is a new collection, after a few weeks people taste are going to change and recommendations won\u0026rsquo;t be relevant anymore. Presentation bias? Let\u0026rsquo;s say you put everything in the hand of the recommender system, and all the items being shown are come from the recommender system. Now if you take the interactions from this site and feed it into another model, it\u0026rsquo;s going to produce the same result as your previous model was predicting! Yes, because the only thing the new system is influenced by, is what your current system is presenting! One way to overcome this issue is to give your users a set of random data and use that piece of random data as an unbiased evaluation set. Another way could be to log what you\u0026rsquo;ve shown to the user and map what they\u0026rsquo;ve seen and what they haven\u0026rsquo;t bought. These sort of unbiased data could help to boost your system out of the local minimum it might be in.\n3. Engineering Architecture Usually, in ML you train and evaluate the model offline, and once you have the model you spin up a service that accepts the features and returns a prediction. So at anytime your service asks for a recommendation item, it goes out to the database search some features and put it in a JSON, send it back to your service. But the story is different in RS where you are going to scoring hundreds of thousands of data. Also, Ranking is computationally intensive, imagine a website like Pinterest where it has hundreds of millions of items and you want to recommend the most relevant pins. This means you need to score lots of things, you need to generate candidates from millions of possibilities and score tens o thousands of candidates. The way items are going to be represented is in multi-dimensional vectors and if a single item is 256 floats that\u0026rsquo;s kilobyte on its own, thousands of it would be a megabyte but you need to score hundreds of thousands and serialize them and send it over the wire! That\u0026rsquo;d be a nightmare for your users. One way to overcome this is that your model has to live in the same box with your data due to serialization overhead. Another point is you don\u0026rsquo;t need to compute everything! Put some factor like some items are a year old so we don\u0026rsquo;t need to score them. In another hand, you can use some sort of clever space partitioning technique where you have these latent representations and then you can carefully slice this space into subsets where you know that similar items live and then when you want to retrieve related products just go to that subspace and not have to scan the whole thing.\nPrecompute can\u0026rsquo;t help! Most of the prediction has to happen online, so you can\u0026rsquo;t define a system that goes to your database, compute the recommendation and when a user browses on data it just takes the recommendation from some sort of cache or database. For example, the way YouTube does it is as a new user comes in and click on video A and the video B and then video C, it folds those interactions and updates the model to serve the users with recommendations. That means not only your predictions have to be online but also your fitting system. So often what you have is prediction system is online and the fitting system always there in the background churning and ingesting new information and update the presentation. So you don\u0026rsquo;t just train a model and put it into production, the data is always changing and you need to have tools to monitor these changes.\n","permalink":"https://alirezasamar.com/posts/three-aspects-of-recommender-systems-in-production/","summary":"Recommender systems are everywhere. They are helpful but sometimes biased. But building a recommender system which is for sure an application of machine learning in production requires some special treatments and aspects to cover in comparison to other machine learning application like let\u0026rsquo;s say classification.\nIn this article I will go through three of these aspects:\nPhoto by Javier Allegue Barros\n1. Model Selection How do you choose the model?","title":"Three Aspects of Recommender Systems in Production"}]